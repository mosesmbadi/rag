# Model/runtime configuration
# Choose LLM provider: 'gemini' or 'local'
LLM_PROVIDER=gemini

# Gemini API configuration
GEMINI_API_KEY=your-gemini-api-key
GEMINI_MODEL=gemini-2.5-flash

# Local model configuration (used when LLM_PROVIDER=local)
LLM_MODEL_CPU_FALLBACK=TinyLlama/TinyLlama-1.1B-Chat-v1.0
LLM_LOAD_IN_8BIT=0

# Maximum length for generated answers
MAX_ANSWER_LENGTH=500

# Data directory containing files to index (absolute or relative to project root)
# Supports PDF and CSV files; scanned recursively including subdirectories
# Defaults to ./data if not set
DATA_DIR=./data/docs

# CSV chunking: max rows per chunk (actual chunk size also limited by CHUNK_MAX_TOKENS)
CSV_ROWS_PER_CHUNK=50