# Model/runtime configuration
# Choose LLM provider: 'gemini' or 'local'
LLM_PROVIDER=gemini

# Gemini API configuration
GEMINI_API_KEY=your-gemini-api-key
GEMINI_MODEL=gemini-2.5-flash

# Local model configuration (used when LLM_PROVIDER=local)
LLM_MODEL_CPU_FALLBACK=TinyLlama/TinyLlama-1.1B-Chat-v1.0
LLM_LOAD_IN_8BIT=0

# Maximum length for generated answers
MAX_ANSWER_LENGTH=500